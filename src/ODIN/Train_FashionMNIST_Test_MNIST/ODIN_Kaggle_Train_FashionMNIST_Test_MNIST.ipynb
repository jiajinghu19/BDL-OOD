{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afa693ec",
   "metadata": {
    "papermill": {
     "duration": 0.006635,
     "end_time": "2022-11-22T04:09:44.499216",
     "exception": false,
     "start_time": "2022-11-22T04:09:44.492581",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ODIN Kaggle - Train on Fashion MNIST and Test on MNIST\n",
    "\n",
    "This notebook is set up to run on Kaggle since they have free GPU hours. At a high level, this notebook\n",
    "- Clones a forked repo of ODIN since their code has some bugs in Python 3 (I'm assuming their syntax was valid in Python 2 or something)\n",
    "- Downloads our model trained on Fashion MNIST\n",
    "- Downloads MNIST and Fashion MNIST datasets\n",
    "- Evaluates the model using ODIN and using the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36aa7e80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:09:44.511327Z",
     "iopub.status.busy": "2022-11-22T04:09:44.510817Z",
     "iopub.status.idle": "2022-11-22T04:09:48.953200Z",
     "shell.execute_reply": "2022-11-22T04:09:48.952020Z"
    },
    "papermill": {
     "duration": 4.450747,
     "end_time": "2022-11-22T04:09:48.955818",
     "exception": false,
     "start_time": "2022-11-22T04:09:44.505071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'BDL-OOD'...\r\n",
      "remote: Enumerating objects: 273, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (273/273), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (199/199), done.\u001b[K\r\n",
      "remote: Total 273 (delta 114), reused 218 (delta 63), pack-reused 0\u001b[K\r\n",
      "Receiving objects: 100% (273/273), 35.91 MiB | 29.53 MiB/s, done.\r\n",
      "Resolving deltas: 100% (114/114), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/jiajinghu19/BDL-OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ca4e0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:09:48.965376Z",
     "iopub.status.busy": "2022-11-22T04:09:48.965067Z",
     "iopub.status.idle": "2022-11-22T04:09:50.837428Z",
     "shell.execute_reply": "2022-11-22T04:09:50.836337Z"
    },
    "papermill": {
     "duration": 1.879765,
     "end_time": "2022-11-22T04:09:50.839768",
     "exception": false,
     "start_time": "2022-11-22T04:09:48.960003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/BDL-OOD/src/ODIN/odin_fork/models\n",
      "Densenet_Train_FashionMNIST_6.15_Percent_Error.pth\r\n"
     ]
    }
   ],
   "source": [
    "# move the pre-SVHN-trained model here\n",
    "%cd /kaggle/working/BDL-OOD/src/ODIN/odin_fork/models\n",
    "!mv ../../Densenet_Train_FashionMNIST_Kaggle/Densenet_Train_FashionMNIST_6.15_Percent_Error.pth ./\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a94e4d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:09:50.849391Z",
     "iopub.status.busy": "2022-11-22T04:09:50.849078Z",
     "iopub.status.idle": "2022-11-22T04:09:52.716932Z",
     "shell.execute_reply": "2022-11-22T04:09:52.715636Z"
    },
    "papermill": {
     "duration": 1.875682,
     "end_time": "2022-11-22T04:09:52.719571",
     "exception": false,
     "start_time": "2022-11-22T04:09:50.843889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/BDL-OOD/src/ODIN/odin_fork/code\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/BDL-OOD/src/ODIN/odin_fork/code\n",
    "!rm densenet.py # delete the densenet.py file\n",
    "!mv ../../Densenet_Train_FashionMNIST_Kaggle/densenet.py ./ # move the correct densenet.py file here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae36aa3",
   "metadata": {
    "papermill": {
     "duration": 0.003765,
     "end_time": "2022-11-22T04:09:52.727563",
     "exception": false,
     "start_time": "2022-11-22T04:09:52.723798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Editing `cal.py`\n",
    "\n",
    "We need to edit `cal.py` in order to\n",
    "- Load the model via load_state_dict\n",
    "- Make sure the data is not normalized, since the model we are using was not trained on normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73b68495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:09:52.737315Z",
     "iopub.status.busy": "2022-11-22T04:09:52.736997Z",
     "iopub.status.idle": "2022-11-22T04:09:52.747730Z",
     "shell.execute_reply": "2022-11-22T04:09:52.746355Z"
    },
    "papermill": {
     "duration": 0.018096,
     "end_time": "2022-11-22T04:09:52.749712",
     "exception": false,
     "start_time": "2022-11-22T04:09:52.731616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cal.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cal.py\n",
    "# %load cal.py\n",
    "# Copyright (c) 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "Created on Sat Sep 19 20:55:56 2015\n",
    "\n",
    "@author: liangshiyu\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy import misc\n",
    "import calMetric as m\n",
    "import calData as d\n",
    "#CUDA_DEVICE = 0\n",
    "\n",
    "start = time.time()\n",
    "#loading data sets\n",
    "\n",
    "def get_transform(dataset_name=\"\"):\n",
    "    # calData.py uses hardcoded image transforms so I'm (Harry) leaving this for now\n",
    "    normalize_transform = transforms.Normalize( # default is CIFAR-10\n",
    "        (125.3/255, 123.0/255, 113.9/255),\n",
    "        (63.0/255, 62.1/255.0, 66.7/255.0)\n",
    "    )\n",
    "    # if dataset_name == \"SVHN\":\n",
    "    #     normalize_transform = transforms.Normalize(\n",
    "    #         (0.43768218, 0.44376934, 0.47280428), \n",
    "    #         (0.1980301, 0.2010157, 0.19703591)\n",
    "    #     )\n",
    "    if dataset_name == \"MNIST\" or dataset_name == \"FashionMNIST\":\n",
    "        normalize_transform = transforms.Normalize((0.2860402,), (0.3530239,)) # this line is important because the model was trained on this data normalization\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.CenterCrop((32)),\n",
    "        normalize_transform\n",
    "    ])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def test(nnName, in_dataset_name, out_data_name, CUDA_DEVICE, epsilon, temperature):\n",
    "    net1 = torch.load(\"../models/{}.pth\".format(nnName))\n",
    "    optimizer1 = optim.SGD(net1.parameters(), lr = 0, momentum = 0)\n",
    "    net1.cuda(CUDA_DEVICE)\n",
    "    net1.eval() # https://ai-pool.com/d/pytorch---error--expected-more-than-1-value-per-channel-when-training\n",
    "    \n",
    "    testset_out = None\n",
    "    testloader_out = None\n",
    "    if out_data_name != \"Uniform\" and out_data_name != \"Gaussian\": # if the test data is not unniform or gaussian\n",
    "        if out_data_name == \"CIFAR-10\": \n",
    "            testset_out = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=get_transform(\"CIFAR-10\"))\n",
    "        elif out_data_name == \"SVHN\": \n",
    "            testset_out = torchvision.datasets.SVHN(root='svhn', split='test', download=True, transform=get_transform(\"SVHN\"))\n",
    "        elif out_data_name == \"MNIST\": \n",
    "            testset_out = torchvision.datasets.MNIST(root='mnist', train=False, download=True, transform=get_transform(\"MNIST\"))  \n",
    "        elif out_data_name == \"FashionMNIST\": \n",
    "            testset_out = torchvision.datasets.FashionMNIST(root='fashionmnist', train=False, download=True, transform=get_transform(\"FashionMNIST\"))                               \n",
    "        else:\n",
    "            testset_out = torchvision.datasets.ImageFolder(\"../data/{}\".format(out_data_name), transform=get_transform(\"CIFAR-10\")) # load the data from the folder\n",
    "        testloader_out = torch.utils.data.DataLoader(testset_out, batch_size=1,\n",
    "                                            shuffle=False, num_workers=2)\n",
    "        \n",
    "    testset_in = None\n",
    "    if in_dataset_name == \"CIFAR-10\": \n",
    "        testset_in = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=get_transform(\"CIFAR-10\"))\n",
    "    elif in_dataset_name == \"CIFAR-100\": \n",
    "        testset_in = torchvision.datasets.CIFAR100(root='../data', train=False, download=True, transform=get_transform(\"CIFAR-10\"))\n",
    "    elif in_dataset_name == \"SVHN\":\n",
    "        testset_in = torchvision.datasets.SVHN(root='svhn', split='test', download=True, transform=get_transform(\"SVHN\"))\n",
    "    elif in_dataset_name == \"MNIST\":\n",
    "        testset_in = torchvision.datasets.MNIST(root='mnist', train=False, download=True, transform=get_transform(\"MNIST\"))\n",
    "    elif in_dataset_name == \"FashionMNIST\":\n",
    "        testset_in = torchvision.datasets.FashionMNIST(root='fashionmnist', train=False, download=True, transform=get_transform(\"FashionMNIST\"))\n",
    "    else:\n",
    "        print(\"Invalid in-distribution dataset name\")\n",
    "    testloader_in = torch.utils.data.DataLoader(testset_in, batch_size=1,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "    \n",
    "    if out_data_name == \"Gaussian\":\n",
    "        d.testGaussian(net1, criterion, CUDA_DEVICE, testloader_in, testloader_in, nnName, out_data_name, epsilon, temperature)\n",
    "        m.metric(nnName, in_dataset_name, out_data_name)\n",
    "\n",
    "    elif out_data_name == \"Uniform\":\n",
    "        d.testUni(net1, criterion, CUDA_DEVICE, testloader_in, testloader_in, nnName, out_data_name, epsilon, temperature)\n",
    "        m.metric(nnName, in_dataset_name, out_data_name)\n",
    "    else:\n",
    "        d.testData(net1, criterion, CUDA_DEVICE, testloader_in, testloader_out, nnName, in_dataset_name, out_data_name, epsilon, temperature) \n",
    "        m.metric(nnName, in_dataset_name, out_data_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b27c51a",
   "metadata": {
    "papermill": {
     "duration": 0.003835,
     "end_time": "2022-11-22T04:09:52.757513",
     "exception": false,
     "start_time": "2022-11-22T04:09:52.753678",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Editing `calData.py`\n",
    "\n",
    "We need to edit `calData.py` in order to\n",
    "- Make sure the model output is shaped properly to fit the ODIN evaluation code\n",
    "- Make sure the data is not normalized, since the model we are using was not trained on normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39310b41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:09:52.767455Z",
     "iopub.status.busy": "2022-11-22T04:09:52.766689Z",
     "iopub.status.idle": "2022-11-22T04:09:52.782008Z",
     "shell.execute_reply": "2022-11-22T04:09:52.780558Z"
    },
    "papermill": {
     "duration": 0.023118,
     "end_time": "2022-11-22T04:09:52.784508",
     "exception": false,
     "start_time": "2022-11-22T04:09:52.761390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting calData.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile calData.py\n",
    "# %load calData.py\n",
    "# Copyright (c) 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "Created on Sat Sep 19 20:55:56 2015\n",
    "\n",
    "@author: liangshiyu\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# this reshape is important because some densenet models output in a shape (10) instead if (1,10)\n",
    "def reshape_output(output):\n",
    "    return torch.reshape(output, (1, 10))\n",
    "\n",
    "def testData(net1, criterion, CUDA_DEVICE, testloader_in, testloader_out, nnName, in_data_name, out_data_name, noiseMagnitude1, temper):\n",
    "    t0 = time.time()\n",
    "    f1 = open(\"./softmax_scores/confidence_Base_In.txt\", 'w')\n",
    "    f2 = open(\"./softmax_scores/confidence_Base_Out.txt\", 'w')\n",
    "    g1 = open(\"./softmax_scores/confidence_Our_In.txt\", 'w')\n",
    "    g2 = open(\"./softmax_scores/confidence_Our_Out.txt\", 'w')\n",
    "    N = 10000\n",
    "    if out_data_name == \"iSUN\":\n",
    "        N = 8925\n",
    "        print(\"Processing in-distribution images\")\n",
    "########################################In-distribution###########################################\n",
    "    for j, data in enumerate(testloader_in):\n",
    "        if j<1000: continue\n",
    "        images, _ = data\n",
    "        \n",
    "        inputs = Variable(images.cuda(CUDA_DEVICE), requires_grad = True)\n",
    "        outputs = reshape_output(net1(inputs))\n",
    "        \n",
    "\n",
    "        # Calculating the confidence of the output, no perturbation added here, no temperature scaling used\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        f1.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temper\n",
    "\t\n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = np.argmax(nnOutputs)\n",
    "        labels = Variable(torch.LongTensor([maxIndexTemp]).cuda(CUDA_DEVICE))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient =  torch.ge(inputs.grad.data, 0)\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        # Normalizing the gradient to the same space of image\n",
    "#         gradient[0][0] = (gradient[0][0] )/(0.5)\n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\n",
    "        outputs = reshape_output(net1(Variable(tempInputs)))\n",
    "        outputs = outputs / temper\n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        g1.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        if j % 100 == 99:\n",
    "            print(\"{:4}/{:4} images processed, {:.1f} seconds used.\".format(j+1-1000, N-1000, time.time()-t0))\n",
    "            t0 = time.time()\n",
    "        \n",
    "        if j == N - 1: break\n",
    "\n",
    "\n",
    "    t0 = time.time()\n",
    "    print(\"Processing out-of-distribution images\")\n",
    "###################################Out-of-Distributions#####################################\n",
    "    for j, data in enumerate(testloader_out):\n",
    "        if j<1000: continue\n",
    "        images, _ = data\n",
    "    \n",
    "    \n",
    "        inputs = Variable(images.cuda(CUDA_DEVICE), requires_grad = True)\n",
    "        outputs = reshape_output(net1(inputs))\n",
    "        \n",
    "\n",
    "\n",
    "        # Calculating the confidence of the output, no perturbation added here\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        f2.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temper\n",
    "  \n",
    "  \n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = np.argmax(nnOutputs)\n",
    "        labels = Variable(torch.LongTensor([maxIndexTemp]).cuda(CUDA_DEVICE))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient =  (torch.ge(inputs.grad.data, 0))\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        # Normalizing the gradient to the same space of image\n",
    "#         gradient[0][0] = (gradient[0][0] )/(0.5)\n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\n",
    "        outputs = reshape_output(net1(Variable(tempInputs)))\n",
    "        outputs = outputs / temper\n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        g2.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        if j % 100 == 99:\n",
    "            print(\"{:4}/{:4} images processed, {:.1f} seconds used.\".format(j+1-1000, N-1000, time.time()-t0))\n",
    "            t0 = time.time()\n",
    "\n",
    "        if j== N-1: break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def testGaussian(net1, criterion, CUDA_DEVICE, testloader_in, testloader_out, nnName, out_data_name, noiseMagnitude1, temper):\n",
    "    t0 = time.time()\n",
    "    f1 = open(\"./softmax_scores/confidence_Base_In.txt\", 'w')\n",
    "    f2 = open(\"./softmax_scores/confidence_Base_Out.txt\", 'w')\n",
    "    g1 = open(\"./softmax_scores/confidence_Our_In.txt\", 'w')\n",
    "    g2 = open(\"./softmax_scores/confidence_Our_Out.txt\", 'w')\n",
    "########################################In-Distribution###############################################\n",
    "    N = 10000\n",
    "    print(\"Processing in-distribution images\")\n",
    "    for j, data in enumerate(testloader_in):\n",
    "        \n",
    "        if j<1000: continue\n",
    "        images, _ = data\n",
    "        \n",
    "        inputs = Variable(images.cuda(CUDA_DEVICE), requires_grad = True)\n",
    "        outputs = net1(inputs)\n",
    "        \n",
    "        \n",
    "        # Calculating the confidence of the output, no perturbation added here\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        f1.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temper\n",
    "        \n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = np.argmax(nnOutputs)\n",
    "        labels = Variable(torch.LongTensor([maxIndexTemp]).cuda(CUDA_DEVICE))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient =  (torch.ge(inputs.grad.data, 0))\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        # Normalizing the gradient to the same space of image\n",
    "        gradient[0][0] = (gradient[0][0] )/(63.0/255.0)\n",
    "        gradient[0][1] = (gradient[0][1] )/(62.1/255.0)\n",
    "        gradient[0][2] = (gradient[0][2])/(66.7/255.0)\n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\n",
    "        outputs = net1(Variable(tempInputs))\n",
    "        outputs = outputs / temper\n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "\n",
    "        g1.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        if j % 100 == 99:\n",
    "            print(\"{:4}/{:4} images processed, {:.1f} seconds used.\".format(j+1-1000, N-1000, time.time()-t0))\n",
    "            t0 = time.time()\n",
    "\n",
    "    \n",
    "    \n",
    "########################################Out-of-Distribution######################################\n",
    "    print(\"Processing out-of-distribution images\")\n",
    "    for j, data in enumerate(testloader_out):\n",
    "        if j<1000: continue\n",
    "        \n",
    "        images = torch.randn(1,3,32,32) + 0.5\n",
    "        images = torch.clamp(images, 0, 1)\n",
    "        images[0][0] = (images[0][0] - 125.3/255) / (63.0/255)\n",
    "        images[0][1] = (images[0][1] - 123.0/255) / (62.1/255)\n",
    "        images[0][2] = (images[0][2] - 113.9/255) / (66.7/255)\n",
    "        \n",
    "        \n",
    "        inputs = Variable(images.cuda(CUDA_DEVICE), requires_grad = True)\n",
    "        outputs = net1(inputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Calculating the confidence of the output, no perturbation added here\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        f2.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temper\n",
    "        \n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = np.argmax(nnOutputs)\n",
    "        labels = Variable(torch.LongTensor([maxIndexTemp]).cuda(CUDA_DEVICE))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient =  (torch.ge(inputs.grad.data, 0))\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        # Normalizing the gradient to the same space of image\n",
    "        gradient[0][0] = (gradient[0][0] )/(63.0/255.0)\n",
    "        gradient[0][1] = (gradient[0][1] )/(62.1/255.0)\n",
    "        gradient[0][2] = (gradient[0][2])/(66.7/255.0)\n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\n",
    "        outputs = net1(Variable(tempInputs))\n",
    "        outputs = outputs / temper\n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        g2.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        \n",
    "        if j % 100 == 99:\n",
    "            print(\"{:4}/{:4} images processed, {:.1f} seconds used.\".format(j+1-1000, N-1000, time.time()-t0))\n",
    "            t0 = time.time()\n",
    "\n",
    "        if j== N-1: break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def testUni(net1, criterion, CUDA_DEVICE, testloader_in, testloader_out, nnName, out_data_name, noiseMagnitude1, temper):\n",
    "    t0 = time.time()\n",
    "    f1 = open(\"./softmax_scores/confidence_Base_In.txt\", 'w')\n",
    "    f2 = open(\"./softmax_scores/confidence_Base_Out.txt\", 'w')\n",
    "    g1 = open(\"./softmax_scores/confidence_Our_In.txt\", 'w')\n",
    "    g2 = open(\"./softmax_scores/confidence_Our_Out.txt\", 'w')\n",
    "########################################In-Distribution###############################################\n",
    "    N = 10000\n",
    "    print(\"Processing in-distribution images\")\n",
    "    for j, data in enumerate(testloader_in):\n",
    "        if j<1000: continue\n",
    "        \n",
    "        images, _ = data\n",
    "        \n",
    "        inputs = Variable(images.cuda(CUDA_DEVICE), requires_grad = True)\n",
    "        outputs = net1(inputs)\n",
    "        \n",
    "        \n",
    "        # Calculating the confidence of the output, no perturbation added here\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        f1.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temper\n",
    "        \n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = np.argmax(nnOutputs)\n",
    "        labels = Variable(torch.LongTensor([maxIndexTemp]).cuda(CUDA_DEVICE))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient =  (torch.ge(inputs.grad.data, 0))\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        # Normalizing the gradient to the same space of image\n",
    "        gradient[0][0] = (gradient[0][0] )/(63.0/255.0)\n",
    "        gradient[0][1] = (gradient[0][1] )/(62.1/255.0)\n",
    "        gradient[0][2] = (gradient[0][2])/(66.7/255.0)\n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\n",
    "        outputs = net1(Variable(tempInputs))\n",
    "        outputs = outputs / temper\n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "\n",
    "        g1.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        if j % 100 == 99:\n",
    "            print(\"{:4}/{:4}  images processed, {:.1f} seconds used.\".format(j+1-1000, N-1000, time.time()-t0))\n",
    "            t0 = time.time()\n",
    "\n",
    "\n",
    "\n",
    "########################################Out-of-Distribution######################################\n",
    "    print(\"Processing out-of-distribution images\")\n",
    "    for j, data in enumerate(testloader_out):\n",
    "        if j<1000: continue\n",
    "        \n",
    "        images = torch.rand(1,3,32,32)\n",
    "        images[0][0] = (images[0][0] - 125.3/255) / (63.0/255)\n",
    "        images[0][1] = (images[0][1] - 123.0/255) / (62.1/255)\n",
    "        images[0][2] = (images[0][2] - 113.9/255) / (66.7/255)\n",
    "        \n",
    "        \n",
    "        inputs = Variable(images.cuda(CUDA_DEVICE), requires_grad = True)\n",
    "        outputs = net1(inputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Calculating the confidence of the output, no perturbation added here\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        f2.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temper\n",
    "        \n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = np.argmax(nnOutputs)\n",
    "        labels = Variable(torch.LongTensor([maxIndexTemp]).cuda(CUDA_DEVICE))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient =  (torch.ge(inputs.grad.data, 0))\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        # Normalizing the gradient to the same space of image\n",
    "        gradient[0][0] = (gradient[0][0] )/(63.0/255.0)\n",
    "        gradient[0][1] = (gradient[0][1] )/(62.1/255.0)\n",
    "        gradient[0][2] = (gradient[0][2])/(66.7/255.0)\n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\n",
    "        outputs = net1(Variable(tempInputs))\n",
    "        outputs = outputs / temper\n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        g2.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        if j % 100 == 99:\n",
    "            print(\"{:4}/{:4} images processed, {:.1f} seconds used.\".format(j+1-1000, N-1000, time.time()-t0))\n",
    "            t0 = time.time()\n",
    "\n",
    "        if j== N-1: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97ac0118",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:09:52.794452Z",
     "iopub.status.busy": "2022-11-22T04:09:52.793706Z",
     "iopub.status.idle": "2022-11-22T04:26:03.380347Z",
     "shell.execute_reply": "2022-11-22T04:26:03.379161Z"
    },
    "papermill": {
     "duration": 970.593952,
     "end_time": "2022-11-22T04:26:03.382858",
     "exception": false,
     "start_time": "2022-11-22T04:09:52.788906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\r\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist/MNIST/raw/train-images-idx3-ubyte.gz\r\n",
      "9913344it [00:00, 31182563.38it/s]                                              \r\n",
      "Extracting mnist/MNIST/raw/train-images-idx3-ubyte.gz to mnist/MNIST/raw\r\n",
      "\r\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\r\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist/MNIST/raw/train-labels-idx1-ubyte.gz\r\n",
      "29696it [00:00, 36915842.20it/s]                                                \r\n",
      "Extracting mnist/MNIST/raw/train-labels-idx1-ubyte.gz to mnist/MNIST/raw\r\n",
      "\r\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\r\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\r\n",
      "1649664it [00:00, 38648444.18it/s]                                              \r\n",
      "Extracting mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw\r\n",
      "\r\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\r\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\r\n",
      "5120it [00:00, 30374591.91it/s]                                                 \r\n",
      "Extracting mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw\r\n",
      "\r\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\r\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to fashionmnist/FashionMNIST/raw/train-images-idx3-ubyte.gz\r\n",
      "26422272it [00:02, 11696294.05it/s]                                             \r\n",
      "Extracting fashionmnist/FashionMNIST/raw/train-images-idx3-ubyte.gz to fashionmnist/FashionMNIST/raw\r\n",
      "\r\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\r\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to fashionmnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz\r\n",
      "29696it [00:00, 209400.70it/s]                                                  \r\n",
      "Extracting fashionmnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz to fashionmnist/FashionMNIST/raw\r\n",
      "\r\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\r\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to fashionmnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\r\n",
      "4422656it [00:01, 3913818.14it/s]                                               \r\n",
      "Extracting fashionmnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to fashionmnist/FashionMNIST/raw\r\n",
      "\r\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\r\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to fashionmnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\r\n",
      "6144it [00:00, 22585279.38it/s]                                                 \r\n",
      "Extracting fashionmnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to fashionmnist/FashionMNIST/raw\r\n",
      "\r\n",
      "/kaggle/working/BDL-OOD/src/ODIN/odin_fork/code/densenet.py:115: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\r\n",
      "  out = F.log_softmax(self.fc(out))\r\n",
      "/kaggle/working/BDL-OOD/src/ODIN/odin_fork/code/calData.py:70: UserWarning: This overload of add is deprecated:\r\n",
      "\tadd(Tensor input, Number alpha, Tensor other, *, Tensor out)\r\n",
      "Consider using one of the following signatures instead:\r\n",
      "\tadd(Tensor input, Tensor other, *, Number alpha, Tensor out) (Triggered internally at  /usr/local/src/pytorch/torch/csrc/utils/python_arg_parser.cpp:1055.)\r\n",
      "  tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\r\n",
      " 100/9000 images processed, 14.0 seconds used.\r\n",
      " 200/9000 images processed, 4.7 seconds used.\r\n",
      " 300/9000 images processed, 5.1 seconds used.\r\n",
      " 400/9000 images processed, 4.7 seconds used.\r\n",
      " 500/9000 images processed, 5.2 seconds used.\r\n",
      " 600/9000 images processed, 4.8 seconds used.\r\n",
      " 700/9000 images processed, 4.7 seconds used.\r\n",
      " 800/9000 images processed, 4.9 seconds used.\r\n",
      " 900/9000 images processed, 4.7 seconds used.\r\n",
      "1000/9000 images processed, 5.1 seconds used.\r\n",
      "1100/9000 images processed, 5.2 seconds used.\r\n",
      "1200/9000 images processed, 4.9 seconds used.\r\n",
      "1300/9000 images processed, 4.6 seconds used.\r\n",
      "1400/9000 images processed, 4.7 seconds used.\r\n",
      "1500/9000 images processed, 4.9 seconds used.\r\n",
      "1600/9000 images processed, 4.7 seconds used.\r\n",
      "1700/9000 images processed, 4.9 seconds used.\r\n",
      "1800/9000 images processed, 5.1 seconds used.\r\n",
      "1900/9000 images processed, 4.9 seconds used.\r\n",
      "2000/9000 images processed, 4.7 seconds used.\r\n",
      "2100/9000 images processed, 4.8 seconds used.\r\n",
      "2200/9000 images processed, 4.7 seconds used.\r\n",
      "2300/9000 images processed, 4.8 seconds used.\r\n",
      "2400/9000 images processed, 5.3 seconds used.\r\n",
      "2500/9000 images processed, 4.7 seconds used.\r\n",
      "2600/9000 images processed, 4.9 seconds used.\r\n",
      "2700/9000 images processed, 4.7 seconds used.\r\n",
      "2800/9000 images processed, 5.0 seconds used.\r\n",
      "2900/9000 images processed, 4.6 seconds used.\r\n",
      "3000/9000 images processed, 4.8 seconds used.\r\n",
      "3100/9000 images processed, 5.0 seconds used.\r\n",
      "3200/9000 images processed, 4.7 seconds used.\r\n",
      "3300/9000 images processed, 4.9 seconds used.\r\n",
      "3400/9000 images processed, 4.7 seconds used.\r\n",
      "3500/9000 images processed, 5.0 seconds used.\r\n",
      "3600/9000 images processed, 4.7 seconds used.\r\n",
      "3700/9000 images processed, 5.5 seconds used.\r\n",
      "3800/9000 images processed, 4.7 seconds used.\r\n",
      "3900/9000 images processed, 4.9 seconds used.\r\n",
      "4000/9000 images processed, 4.7 seconds used.\r\n",
      "4100/9000 images processed, 4.8 seconds used.\r\n",
      "4200/9000 images processed, 4.8 seconds used.\r\n",
      "4300/9000 images processed, 4.7 seconds used.\r\n",
      "4400/9000 images processed, 5.3 seconds used.\r\n",
      "4500/9000 images processed, 4.6 seconds used.\r\n",
      "4600/9000 images processed, 4.8 seconds used.\r\n",
      "4700/9000 images processed, 4.7 seconds used.\r\n",
      "4800/9000 images processed, 4.7 seconds used.\r\n",
      "4900/9000 images processed, 4.8 seconds used.\r\n",
      "5000/9000 images processed, 5.1 seconds used.\r\n",
      "5100/9000 images processed, 4.9 seconds used.\r\n",
      "5200/9000 images processed, 4.7 seconds used.\r\n",
      "5300/9000 images processed, 4.9 seconds used.\r\n",
      "5400/9000 images processed, 4.7 seconds used.\r\n",
      "5500/9000 images processed, 4.9 seconds used.\r\n",
      "5600/9000 images processed, 4.6 seconds used.\r\n",
      "5700/9000 images processed, 5.1 seconds used.\r\n",
      "5800/9000 images processed, 4.9 seconds used.\r\n",
      "5900/9000 images processed, 4.7 seconds used.\r\n",
      "6000/9000 images processed, 4.9 seconds used.\r\n",
      "6100/9000 images processed, 4.7 seconds used.\r\n",
      "6200/9000 images processed, 4.9 seconds used.\r\n",
      "6300/9000 images processed, 5.1 seconds used.\r\n",
      "6400/9000 images processed, 4.7 seconds used.\r\n",
      "6500/9000 images processed, 4.8 seconds used.\r\n",
      "6600/9000 images processed, 5.0 seconds used.\r\n",
      "6700/9000 images processed, 5.0 seconds used.\r\n",
      "6800/9000 images processed, 4.6 seconds used.\r\n",
      "6900/9000 images processed, 4.9 seconds used.\r\n",
      "7000/9000 images processed, 5.1 seconds used.\r\n",
      "7100/9000 images processed, 4.8 seconds used.\r\n",
      "7200/9000 images processed, 4.7 seconds used.\r\n",
      "7300/9000 images processed, 4.7 seconds used.\r\n",
      "7400/9000 images processed, 4.9 seconds used.\r\n",
      "7500/9000 images processed, 4.6 seconds used.\r\n",
      "7600/9000 images processed, 5.4 seconds used.\r\n",
      "7700/9000 images processed, 4.7 seconds used.\r\n",
      "7800/9000 images processed, 4.9 seconds used.\r\n",
      "7900/9000 images processed, 4.7 seconds used.\r\n",
      "8000/9000 images processed, 4.8 seconds used.\r\n",
      "8100/9000 images processed, 4.6 seconds used.\r\n",
      "8200/9000 images processed, 4.7 seconds used.\r\n",
      "8300/9000 images processed, 5.4 seconds used.\r\n",
      "8400/9000 images processed, 4.7 seconds used.\r\n",
      "8500/9000 images processed, 4.9 seconds used.\r\n",
      "8600/9000 images processed, 4.7 seconds used.\r\n",
      "8700/9000 images processed, 4.9 seconds used.\r\n",
      "8800/9000 images processed, 4.6 seconds used.\r\n",
      "8900/9000 images processed, 5.3 seconds used.\r\n",
      "9000/9000 images processed, 4.7 seconds used.\r\n",
      "Processing out-of-distribution images\r\n",
      " 100/9000 images processed, 7.1 seconds used.\r\n",
      " 200/9000 images processed, 4.7 seconds used.\r\n",
      " 300/9000 images processed, 4.6 seconds used.\r\n",
      " 400/9000 images processed, 4.9 seconds used.\r\n",
      " 500/9000 images processed, 5.1 seconds used.\r\n",
      " 600/9000 images processed, 5.0 seconds used.\r\n",
      " 700/9000 images processed, 4.6 seconds used.\r\n",
      " 800/9000 images processed, 5.0 seconds used.\r\n",
      " 900/9000 images processed, 4.6 seconds used.\r\n",
      "1000/9000 images processed, 4.9 seconds used.\r\n",
      "1100/9000 images processed, 4.7 seconds used.\r\n",
      "1200/9000 images processed, 5.2 seconds used.\r\n",
      "1300/9000 images processed, 4.9 seconds used.\r\n",
      "1400/9000 images processed, 4.7 seconds used.\r\n",
      "1500/9000 images processed, 4.9 seconds used.\r\n",
      "1600/9000 images processed, 4.6 seconds used.\r\n",
      "1700/9000 images processed, 4.9 seconds used.\r\n",
      "1800/9000 images processed, 5.1 seconds used.\r\n",
      "1900/9000 images processed, 4.7 seconds used.\r\n",
      "2000/9000 images processed, 4.7 seconds used.\r\n",
      "2100/9000 images processed, 4.7 seconds used.\r\n",
      "2200/9000 images processed, 4.8 seconds used.\r\n",
      "2300/9000 images processed, 4.6 seconds used.\r\n",
      "2400/9000 images processed, 4.9 seconds used.\r\n",
      "2500/9000 images processed, 5.0 seconds used.\r\n",
      "2600/9000 images processed, 4.8 seconds used.\r\n",
      "2700/9000 images processed, 4.7 seconds used.\r\n",
      "2800/9000 images processed, 4.6 seconds used.\r\n",
      "2900/9000 images processed, 5.0 seconds used.\r\n",
      "3000/9000 images processed, 4.7 seconds used.\r\n",
      "3100/9000 images processed, 5.3 seconds used.\r\n",
      "3200/9000 images processed, 4.7 seconds used.\r\n",
      "3300/9000 images processed, 4.8 seconds used.\r\n",
      "3400/9000 images processed, 4.7 seconds used.\r\n",
      "3500/9000 images processed, 4.6 seconds used.\r\n",
      "3600/9000 images processed, 4.8 seconds used.\r\n",
      "3700/9000 images processed, 4.7 seconds used.\r\n",
      "3800/9000 images processed, 5.4 seconds used.\r\n",
      "3900/9000 images processed, 4.7 seconds used.\r\n",
      "4000/9000 images processed, 4.9 seconds used.\r\n",
      "4100/9000 images processed, 4.6 seconds used.\r\n",
      "4200/9000 images processed, 4.9 seconds used.\r\n",
      "4300/9000 images processed, 4.6 seconds used.\r\n",
      "4400/9000 images processed, 4.8 seconds used.\r\n",
      "4500/9000 images processed, 5.2 seconds used.\r\n",
      "4600/9000 images processed, 4.7 seconds used.\r\n",
      "4700/9000 images processed, 4.9 seconds used.\r\n",
      "4800/9000 images processed, 4.6 seconds used.\r\n",
      "4900/9000 images processed, 4.8 seconds used.\r\n",
      "5000/9000 images processed, 4.6 seconds used.\r\n",
      "5100/9000 images processed, 5.1 seconds used.\r\n",
      "5200/9000 images processed, 4.8 seconds used.\r\n",
      "5300/9000 images processed, 4.7 seconds used.\r\n",
      "5400/9000 images processed, 4.8 seconds used.\r\n",
      "5500/9000 images processed, 4.6 seconds used.\r\n",
      "5600/9000 images processed, 4.8 seconds used.\r\n",
      "5700/9000 images processed, 4.6 seconds used.\r\n",
      "5800/9000 images processed, 5.2 seconds used.\r\n",
      "5900/9000 images processed, 4.8 seconds used.\r\n",
      "6000/9000 images processed, 4.6 seconds used.\r\n",
      "6100/9000 images processed, 4.8 seconds used.\r\n",
      "6200/9000 images processed, 4.6 seconds used.\r\n",
      "6300/9000 images processed, 4.9 seconds used.\r\n",
      "6400/9000 images processed, 5.0 seconds used.\r\n",
      "6500/9000 images processed, 4.8 seconds used.\r\n",
      "6600/9000 images processed, 4.6 seconds used.\r\n",
      "6700/9000 images processed, 4.6 seconds used.\r\n",
      "6800/9000 images processed, 4.8 seconds used.\r\n",
      "6900/9000 images processed, 4.6 seconds used.\r\n",
      "7000/9000 images processed, 4.8 seconds used.\r\n",
      "7100/9000 images processed, 5.1 seconds used.\r\n",
      "7200/9000 images processed, 4.9 seconds used.\r\n",
      "7300/9000 images processed, 4.6 seconds used.\r\n",
      "7400/9000 images processed, 4.6 seconds used.\r\n",
      "7500/9000 images processed, 4.9 seconds used.\r\n",
      "7600/9000 images processed, 4.5 seconds used.\r\n",
      "7700/9000 images processed, 5.3 seconds used.\r\n",
      "7800/9000 images processed, 4.6 seconds used.\r\n",
      "7900/9000 images processed, 5.0 seconds used.\r\n",
      "8000/9000 images processed, 4.6 seconds used.\r\n",
      "8100/9000 images processed, 4.8 seconds used.\r\n",
      "8200/9000 images processed, 4.7 seconds used.\r\n",
      "8300/9000 images processed, 4.6 seconds used.\r\n",
      "8400/9000 images processed, 5.3 seconds used.\r\n",
      "8500/9000 images processed, 4.6 seconds used.\r\n",
      "8600/9000 images processed, 4.8 seconds used.\r\n",
      "8700/9000 images processed, 4.6 seconds used.\r\n",
      "8800/9000 images processed, 4.9 seconds used.\r\n",
      "8900/9000 images processed, 4.5 seconds used.\r\n",
      "9000/9000 images processed, 4.6 seconds used.\r\n",
      "Neural network architecture:                 Densenet\r\n",
      "In-distribution dataset:                 FashionMNIST\r\n",
      "Out-of-distribution dataset:                    MNIST\r\n",
      "\r\n",
      "                          Baseline         Our Method\r\n",
      "FPR at TPR 95%:              60.9%              11.3% \r\n",
      "Detection error:             16.1%               6.9%\r\n",
      "AUROC:                       90.3%              97.9%\r\n",
      "AUPR In:                     92.5%              98.3%\r\n",
      "AUPR Out:                    87.5%              97.5%\r\n"
     ]
    }
   ],
   "source": [
    "!python main.py --nn Densenet_Train_FashionMNIST_6.15_Percent_Error --in_dataset FashionMNIST --out_dataset MNIST --magnitude 0.0014 --temperature 1000 --gpu 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 987.089663,
   "end_time": "2022-11-22T04:26:03.920279",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-22T04:09:36.830616",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
