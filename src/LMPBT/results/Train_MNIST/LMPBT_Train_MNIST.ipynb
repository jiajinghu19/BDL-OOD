{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-11-29T16:21:51.697345Z","iopub.status.busy":"2022-11-29T16:21:51.696466Z","iopub.status.idle":"2022-11-29T16:21:56.044810Z","shell.execute_reply":"2022-11-29T16:21:56.043193Z","shell.execute_reply.started":"2022-11-29T16:21:51.697245Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'BDL-OOD'...\n","remote: Enumerating objects: 350, done.\u001b[K\n","remote: Counting objects: 100% (20/20), done.\u001b[K\n","remote: Compressing objects: 100% (13/13), done.\u001b[K\n","remote: Total 350 (delta 9), reused 16 (delta 7), pack-reused 330\u001b[K\n","Receiving objects: 100% (350/350), 48.31 MiB | 25.10 MiB/s, done.\n","Resolving deltas: 100% (145/145), done.\n"]}],"source":["!git clone https://github.com/jiajinghu19/BDL-OOD.git"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-11-29T16:22:01.106575Z","iopub.status.busy":"2022-11-29T16:22:01.106191Z","iopub.status.idle":"2022-11-29T16:22:02.210520Z","shell.execute_reply":"2022-11-29T16:22:02.209373Z","shell.execute_reply.started":"2022-11-29T16:22:01.106540Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working/BDL-OOD/src/LMBPT_fork\n","DCGAN_VAE_pixel.py  cifar100_netE.pth\t  get_lmpbt_score.py   train_vae.py\n","README.md\t    cifar100_netG.pth\t  get_metrics.py\n","Readme.txt\t    cifar100_nll_vae.npy  hessian3.py\n","cel100_nll_vae.npy  get_eig_vecs_vae.py   svhn100_nll_vae.npy\n"]}],"source":["%cd ./BDL-OOD/src/LMPBT/LMPBT_fork\n","!ls"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-11-29T16:27:03.142402Z","iopub.status.busy":"2022-11-29T16:27:03.141975Z","iopub.status.idle":"2022-11-29T16:28:15.707019Z","shell.execute_reply":"2022-11-29T16:28:15.705812Z","shell.execute_reply.started":"2022-11-29T16:27:03.142367Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Random Seed:  8286\n","epoch:0 recon:5698.080078125 kl:62.21782684326172\n","epoch:0 recon:3220.711238445622 kl:19.345625433591334\n","epoch:0 recon:2425.0663965329604 kl:24.995567331266642\n","epoch:0 recon:2111.534203209354 kl:29.93858082033075\n","epoch:0 recon:1939.5001330292432 kl:33.4285564517737\n","epoch:1 recon:1857.1481647897274 kl:35.233282409830295\n","epoch:1 recon:1768.3676751387748 kl:37.305583435192446\n","epoch:1 recon:1702.303980038771 kl:38.939445361806385\n","epoch:1 recon:1649.898068118405 kl:40.31685946823715\n","epoch:1 recon:1607.3828401412086 kl:41.581914272527584\n","epoch:2 recon:1582.3116124877279 kl:42.39834258147576\n","epoch:2 recon:1550.6254094466208 kl:43.52416829356101\n","epoch:2 recon:1523.2205281111103 kl:44.56933935532139\n","epoch:2 recon:1498.536809568736 kl:45.545583761921804\n","^C\n","Traceback (most recent call last):\n","  File \"train_vae.py\", line 136, in <module>\n","    for i, (x, _) in enumerate(dataloader):\n","  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 530, in __next__\n","    data = self._next_data()\n","  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 570, in _next_data\n","    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n","  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","  File \"/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py\", line 145, in __getitem__\n","    img = self.transform(img)\n","  File \"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n","    img = t(img)\n","  File \"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 135, in __call__\n","    return F.to_tensor(pic)\n","  File \"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py\", line 155, in to_tensor\n","    return img.to(dtype=default_float_dtype).div(255)\n","KeyboardInterrupt\n"]}],"source":["!python train_vae.py --dataset MNIST --nc 1"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":4}
