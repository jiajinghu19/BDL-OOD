{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ODIN Kaggle - Train on MNIST and Test on Fashion MNIST\n\nThis notebook is set up to run on Kaggle since they have free GPU hours. At a high level, this notebook\n- Clones a forked repo of ODIN since their code has some bugs in Python 3 (I'm assuming their syntax was valid in Python 2 or something)\n- Downloads our model trained on MNIST\n- Downloads MNIST and Fashion MNIST datasets\n- Evaluates the model using ODIN and using the baseline","metadata":{"papermill":{"duration":0.004166,"end_time":"2022-11-17T17:37:30.126413","exception":false,"start_time":"2022-11-17T17:37:30.122247","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!git clone https://github.com/jiajinghu19/BDL-OOD","metadata":{"papermill":{"duration":3.923427,"end_time":"2022-11-17T17:37:34.053989","exception":false,"start_time":"2022-11-17T17:37:30.130562","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-11-17T19:13:45.206844Z","iopub.execute_input":"2022-11-17T19:13:45.207349Z","iopub.status.idle":"2022-11-17T19:13:50.011328Z","shell.execute_reply.started":"2022-11-17T19:13:45.207305Z","shell.execute_reply":"2022-11-17T19:13:50.010183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# move the pre-SVHN-trained model here\n%cd /kaggle/working/BDL-OOD/src/ODIN/odin_fork/models\n!mv ../../Densenet_Train_MNIST_Kaggle/DenseNet_Train_MNIST_99.65_accuracy.pth ./\n!ls","metadata":{"papermill":{"duration":1.926345,"end_time":"2022-11-17T17:37:35.983866","exception":false,"start_time":"2022-11-17T17:37:34.057521","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-11-17T19:13:51.884041Z","iopub.execute_input":"2022-11-17T19:13:51.884796Z","iopub.status.idle":"2022-11-17T19:13:53.786674Z","shell.execute_reply.started":"2022-11-17T19:13:51.884756Z","shell.execute_reply":"2022-11-17T19:13:53.785565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/BDL-OOD/src/ODIN/odin_fork/code\n!rm densenet.py # delete the densenet.py file\n!mv ../../Densenet_Train_MNIST_Kaggle/densenet.py ./ # move the correct densenet.py file here","metadata":{"papermill":{"duration":1.899714,"end_time":"2022-11-17T17:37:37.886902","exception":false,"start_time":"2022-11-17T17:37:35.987188","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-11-17T19:13:56.254573Z","iopub.execute_input":"2022-11-17T19:13:56.254955Z","iopub.status.idle":"2022-11-17T19:13:58.216029Z","shell.execute_reply.started":"2022-11-17T19:13:56.254920Z","shell.execute_reply":"2022-11-17T19:13:58.214561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Editing `cal.py`\n\nWe need to edit `cal.py` in order to\n- Load the model via load_state_dict\n- Make sure the data is not normalized, since the model we are using was not trained on normalized data","metadata":{}},{"cell_type":"code","source":"%%writefile cal.py\n# %load cal.py\n# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\n\"\"\"\nCreated on Sat Sep 19 20:55:56 2015\n\n@author: liangshiyu\n\"\"\"\n\nfrom __future__ import print_function\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\nimport time\nfrom scipy import misc\nimport calMetric as m\nimport calData as d\n#CUDA_DEVICE = 0\n\nstart = time.time()\n#loading data sets\n\ndef get_transform(dataset_name=\"\"):\n    # calData.py uses hardcoded image transforms so I'm (Harry) leaving this for now\n    normalize_transform = transforms.Normalize( # default is CIFAR-10\n        (125.3/255, 123.0/255, 113.9/255),\n        (63.0/255, 62.1/255.0, 66.7/255.0)\n    )\n    # if dataset_name == \"SVHN\":\n    #     normalize_transform = transforms.Normalize(\n    #         (0.43768218, 0.44376934, 0.47280428), \n    #         (0.1980301, 0.2010157, 0.19703591)\n    #     )\n    if dataset_name == \"MNIST\" or dataset_name == \"FashionMNIST\":\n        normalize_transform = transforms.Normalize((0,), (1,)) # this line is important because the model we are using was not trained on normalized data\n    return transforms.Compose([\n        transforms.ToTensor(),\n        transforms.CenterCrop((32)),\n        normalize_transform\n    ])\n\ncriterion = nn.CrossEntropyLoss()\n\nfrom densenet import DenseNet121\n\ndef test(nnName, in_dataset_name, out_data_name, CUDA_DEVICE, epsilon, temperature):\n    net1 = DenseNet121(num_classes=10, grayscale=True)\n    net1.load_state_dict(torch.load(\"../models/{}.pth\".format(nnName)))\n    optimizer1 = optim.SGD(net1.parameters(), lr = 0, momentum = 0)\n    net1.cuda(CUDA_DEVICE)\n    net1.eval() # https://ai-pool.com/d/pytorch---error--expected-more-than-1-value-per-channel-when-training\n    \n    testset_out = None\n    testloader_out = None\n    if out_data_name != \"Uniform\" and out_data_name != \"Gaussian\": # if the test data is not unniform or gaussian\n        if out_data_name == \"CIFAR-10\": \n            testset_out = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=get_transform(\"CIFAR-10\"))\n        elif out_data_name == \"SVHN\": \n            testset_out = torchvision.datasets.SVHN(root='svhn', split='test', download=True, transform=get_transform(\"SVHN\"))\n        elif out_data_name == \"MNIST\": \n            testset_out = torchvision.datasets.MNIST(root='mnist', train=False, download=True, transform=get_transform(\"MNIST\"))  \n        elif out_data_name == \"FashionMNIST\": \n            testset_out = torchvision.datasets.FashionMNIST(root='fashionmnist', train=False, download=True, transform=get_transform(\"FashionMNIST\"))                               \n        else:\n            testset_out = torchvision.datasets.ImageFolder(\"../data/{}\".format(out_data_name), transform=get_transform(\"CIFAR-10\")) # load the data from the folder\n        testloader_out = torch.utils.data.DataLoader(testset_out, batch_size=1,\n                                            shuffle=False, num_workers=2)\n        \n    testset_in = None\n    if in_dataset_name == \"CIFAR-10\": \n        testset_in = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=get_transform(\"CIFAR-10\"))\n    elif in_dataset_name == \"CIFAR-100\": \n        testset_in = torchvision.datasets.CIFAR100(root='../data', train=False, download=True, transform=get_transform(\"CIFAR-10\"))\n    elif in_dataset_name == \"SVHN\":\n        testset_in = torchvision.datasets.SVHN(root='svhn', split='test', download=True, transform=get_transform(\"SVHN\"))\n    elif in_dataset_name == \"MNIST\":\n        testset_in = torchvision.datasets.MNIST(root='mnist', train=False, download=True, transform=get_transform(\"MNIST\"))\n    elif in_dataset_name == \"FashionMNIST\":\n        testset_in = torchvision.datasets.FashionMNIST(root='fashionmnist', train=False, download=True, transform=get_transform(\"FashionMNIST\"))\n    else:\n        print(\"Invalid in-distribution dataset name\")\n    testloader_in = torch.utils.data.DataLoader(testset_in, batch_size=1,\n                                        shuffle=False, num_workers=2)\n    \n    if out_data_name == \"Gaussian\":\n        d.testGaussian(net1, criterion, CUDA_DEVICE, testloader_in, testloader_in, nnName, out_data_name, epsilon, temperature)\n        m.metric(nnName, in_dataset_name, out_data_name)\n\n    elif out_data_name == \"Uniform\":\n        d.testUni(net1, criterion, CUDA_DEVICE, testloader_in, testloader_in, nnName, out_data_name, epsilon, temperature)\n        m.metric(nnName, in_dataset_name, out_data_name)\n    else:\n        d.testData(net1, criterion, CUDA_DEVICE, testloader_in, testloader_out, nnName, in_dataset_name, out_data_name, epsilon, temperature) \n        m.metric(nnName, in_dataset_name, out_data_name)\n\n\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-11-17T19:22:25.195720Z","iopub.execute_input":"2022-11-17T19:22:25.196117Z","iopub.status.idle":"2022-11-17T19:22:25.206457Z","shell.execute_reply.started":"2022-11-17T19:22:25.196084Z","shell.execute_reply":"2022-11-17T19:22:25.205387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Editing `calData.py`\n\nWe need to edit `calData.py` in order to\n- Make sure the model output is shaped properly to fit the ODIN evaluation code\n- Make sure the data is not normalized, since the model we are using was not trained on normalized data","metadata":{}},{"cell_type":"code","source":"%%writefile calData.py\n# %load calData.py\n# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\n\"\"\"\nCreated on Sat Sep 19 20:55:56 2015\n\n@author: liangshiyu\n\"\"\"\n\nfrom __future__ import print_function\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\nimport torchvision.transforms as transforms\nimport numpy as np\nimport time\n\n# this reshape is important because some densenet models output in a shape (10) instead if (1,10)\ndef reshape_output(output):\n    return torch.reshape(output, (1, 10))\n\ndef testData(net1, criterion, CUDA_DEVICE, testloader_in, testloader_out, nnName, in_data_name, out_data_name, noiseMagnitude1, temper):\n    t0 = time.time()\n    f1 = open(\"./softmax_scores/confidence_Base_In.txt\", 'w')\n    f2 = open(\"./softmax_scores/confidence_Base_Out.txt\", 'w')\n    g1 = open(\"./softmax_scores/confidence_Our_In.txt\", 'w')\n    g2 = open(\"./softmax_scores/confidence_Our_Out.txt\", 'w')\n    N = 10000\n    if out_data_name == \"iSUN\":\n        N = 8925\n        print(\"Processing in-distribution images\")\n########################################In-distribution###########################################\n    for j, data in enumerate(testloader_in):\n        if j<1000: continue\n        images, _ = data\n        \n        inputs = Variable(images.cuda(CUDA_DEVICE), requires_grad = True)\n        exponents, softmax_values = net1(inputs)\n        outputs = reshape_output(softmax_values)\n        \n\n        # Calculating the confidence of the output, no perturbation added here, no temperature scaling used\n        nnOutputs = outputs.data.cpu()\n        nnOutputs = nnOutputs.numpy()\n        nnOutputs = nnOutputs[0]\n        nnOutputs = nnOutputs - np.max(nnOutputs)\n        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n        f1.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n        \n        # Using temperature scaling\n        outputs = outputs / temper\n\t\n        # Calculating the perturbation we need to add, that is,\n        # the sign of gradient of cross entropy loss w.r.t. input\n        maxIndexTemp = np.argmax(nnOutputs)\n        labels = Variable(torch.LongTensor([maxIndexTemp]).cuda(CUDA_DEVICE))\n        loss = criterion(outputs, labels)\n        loss.backward()\n        \n        # Normalizing the gradient to binary in {0, 1}\n        gradient =  torch.ge(inputs.grad.data, 0)\n        gradient = (gradient.float() - 0.5) * 2\n        # Normalizing the gradient to the same space of image\n#         gradient[0][0] = (gradient[0][0] )/(0.5)\n        # Adding small perturbations to images\n        tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\n        exponents, softmax_values = net1(Variable(tempInputs))\n        outputs = reshape_output(softmax_values)\n        outputs = outputs / temper\n        # Calculating the confidence after adding perturbations\n        nnOutputs = outputs.data.cpu()\n        nnOutputs = nnOutputs.numpy()\n        nnOutputs = nnOutputs[0]\n        nnOutputs = nnOutputs - np.max(nnOutputs)\n        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n        g1.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n        if j % 100 == 99:\n            print(\"{:4}/{:4} images processed, {:.1f} seconds used.\".format(j+1-1000, N-1000, time.time()-t0))\n            t0 = time.time()\n        \n        if j == N - 1: break\n\n\n    t0 = time.time()\n    print(\"Processing out-of-distribution images\")\n###################################Out-of-Distributions#####################################\n    for j, data in enumerate(testloader_out):\n        if j<1000: continue\n        images, _ = data\n    \n    \n        inputs = Variable(images.cuda(CUDA_DEVICE), requires_grad = True)\n        exponents, softmax_values = net1(inputs)\n        outputs = reshape_output(softmax_values)\n        \n\n\n        # Calculating the confidence of the output, no perturbation added here\n        nnOutputs = outputs.data.cpu()\n        nnOutputs = nnOutputs.numpy()\n        nnOutputs = nnOutputs[0]\n        nnOutputs = nnOutputs - np.max(nnOutputs)\n        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n        f2.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n        \n        # Using temperature scaling\n        outputs = outputs / temper\n  \n  \n        # Calculating the perturbation we need to add, that is,\n        # the sign of gradient of cross entropy loss w.r.t. input\n        maxIndexTemp = np.argmax(nnOutputs)\n        labels = Variable(torch.LongTensor([maxIndexTemp]).cuda(CUDA_DEVICE))\n        loss = criterion(outputs, labels)\n        loss.backward()\n        \n        # Normalizing the gradient to binary in {0, 1}\n        gradient =  (torch.ge(inputs.grad.data, 0))\n        gradient = (gradient.float() - 0.5) * 2\n        # Normalizing the gradient to the same space of image\n#         gradient[0][0] = (gradient[0][0] )/(0.5)\n        # Adding small perturbations to images\n        tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\n        exponents, softmax_values = net1(Variable(tempInputs))\n        outputs = reshape_output(softmax_values)\n        outputs = outputs / temper\n        # Calculating the confidence after adding perturbations\n        nnOutputs = outputs.data.cpu()\n        nnOutputs = nnOutputs.numpy()\n        nnOutputs = nnOutputs[0]\n        nnOutputs = nnOutputs - np.max(nnOutputs)\n        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n        g2.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n        if j % 100 == 99:\n            print(\"{:4}/{:4} images processed, {:.1f} seconds used.\".format(j+1-1000, N-1000, time.time()-t0))\n            t0 = time.time()\n\n        if j== N-1: break\n\n\n\n\ndef testGaussian(net1, criterion, CUDA_DEVICE, testloader_in, testloader_out, nnName, out_data_name, noiseMagnitude1, temper):\n    t0 = time.time()\n    f1 = open(\"./softmax_scores/confidence_Base_In.txt\", 'w')\n    f2 = open(\"./softmax_scores/confidence_Base_Out.txt\", 'w')\n    g1 = open(\"./softmax_scores/confidence_Our_In.txt\", 'w')\n    g2 = open(\"./softmax_scores/confidence_Our_Out.txt\", 'w')\n########################################In-Distribution###############################################\n    N = 10000\n    print(\"Processing in-distribution images\")\n    for j, data in enumerate(testloader_in):\n        \n        if j<1000: continue\n        images, _ = data\n        \n        inputs = Variable(images.cuda(CUDA_DEVICE), requires_grad = True)\n        outputs = net1(inputs)\n        \n        \n        # Calculating the confidence of the output, no perturbation added here\n        nnOutputs = outputs.data.cpu()\n        nnOutputs = nnOutputs.numpy()\n        nnOutputs = nnOutputs[0]\n        nnOutputs = nnOutputs - np.max(nnOutputs)\n        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n        f1.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n        \n        # Using temperature scaling\n        outputs = outputs / temper\n        \n        # Calculating the perturbation we need to add, that is,\n        # the sign of gradient of cross entropy loss w.r.t. input\n        maxIndexTemp = np.argmax(nnOutputs)\n        labels = Variable(torch.LongTensor([maxIndexTemp]).cuda(CUDA_DEVICE))\n        loss = criterion(outputs, labels)\n        loss.backward()\n        \n        \n        # Normalizing the gradient to binary in {0, 1}\n        gradient =  (torch.ge(inputs.grad.data, 0))\n        gradient = (gradient.float() - 0.5) * 2\n        # Normalizing the gradient to the same space of image\n        gradient[0][0] = (gradient[0][0] )/(63.0/255.0)\n        gradient[0][1] = (gradient[0][1] )/(62.1/255.0)\n        gradient[0][2] = (gradient[0][2])/(66.7/255.0)\n        # Adding small perturbations to images\n        tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\n        outputs = net1(Variable(tempInputs))\n        outputs = outputs / temper\n        # Calculating the confidence after adding perturbations\n        nnOutputs = outputs.data.cpu()\n        nnOutputs = nnOutputs.numpy()\n        nnOutputs = nnOutputs[0]\n        nnOutputs = nnOutputs - np.max(nnOutputs)\n        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n\n        g1.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n        if j % 100 == 99:\n            print(\"{:4}/{:4} images processed, {:.1f} seconds used.\".format(j+1-1000, N-1000, time.time()-t0))\n            t0 = time.time()\n\n    \n    \n########################################Out-of-Distribution######################################\n    print(\"Processing out-of-distribution images\")\n    for j, data in enumerate(testloader_out):\n        if j<1000: continue\n        \n        images = torch.randn(1,3,32,32) + 0.5\n        images = torch.clamp(images, 0, 1)\n        images[0][0] = (images[0][0] - 125.3/255) / (63.0/255)\n        images[0][1] = (images[0][1] - 123.0/255) / (62.1/255)\n        images[0][2] = (images[0][2] - 113.9/255) / (66.7/255)\n        \n        \n        inputs = Variable(images.cuda(CUDA_DEVICE), requires_grad = True)\n        outputs = net1(inputs)\n        \n        \n        \n        # Calculating the confidence of the output, no perturbation added here\n        nnOutputs = outputs.data.cpu()\n        nnOutputs = nnOutputs.numpy()\n        nnOutputs = nnOutputs[0]\n        nnOutputs = nnOutputs - np.max(nnOutputs)\n        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n        f2.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n        \n        # Using temperature scaling\n        outputs = outputs / temper\n        \n        # Calculating the perturbation we need to add, that is,\n        # the sign of gradient of cross entropy loss w.r.t. input\n        maxIndexTemp = np.argmax(nnOutputs)\n        labels = Variable(torch.LongTensor([maxIndexTemp]).cuda(CUDA_DEVICE))\n        loss = criterion(outputs, labels)\n        loss.backward()\n        \n        # Normalizing the gradient to binary in {0, 1}\n        gradient =  (torch.ge(inputs.grad.data, 0))\n        gradient = (gradient.float() - 0.5) * 2\n        # Normalizing the gradient to the same space of image\n        gradient[0][0] = (gradient[0][0] )/(63.0/255.0)\n        gradient[0][1] = (gradient[0][1] )/(62.1/255.0)\n        gradient[0][2] = (gradient[0][2])/(66.7/255.0)\n        # Adding small perturbations to images\n        tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\n        outputs = net1(Variable(tempInputs))\n        outputs = outputs / temper\n        # Calculating the confidence after adding perturbations\n        nnOutputs = outputs.data.cpu()\n        nnOutputs = nnOutputs.numpy()\n        nnOutputs = nnOutputs[0]\n        nnOutputs = nnOutputs - np.max(nnOutputs)\n        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n        g2.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n        \n        if j % 100 == 99:\n            print(\"{:4}/{:4} images processed, {:.1f} seconds used.\".format(j+1-1000, N-1000, time.time()-t0))\n            t0 = time.time()\n\n        if j== N-1: break\n\n\n\n\ndef testUni(net1, criterion, CUDA_DEVICE, testloader_in, testloader_out, nnName, out_data_name, noiseMagnitude1, temper):\n    t0 = time.time()\n    f1 = open(\"./softmax_scores/confidence_Base_In.txt\", 'w')\n    f2 = open(\"./softmax_scores/confidence_Base_Out.txt\", 'w')\n    g1 = open(\"./softmax_scores/confidence_Our_In.txt\", 'w')\n    g2 = open(\"./softmax_scores/confidence_Our_Out.txt\", 'w')\n########################################In-Distribution###############################################\n    N = 10000\n    print(\"Processing in-distribution images\")\n    for j, data in enumerate(testloader_in):\n        if j<1000: continue\n        \n        images, _ = data\n        \n        inputs = Variable(images.cuda(CUDA_DEVICE), requires_grad = True)\n        outputs = net1(inputs)\n        \n        \n        # Calculating the confidence of the output, no perturbation added here\n        nnOutputs = outputs.data.cpu()\n        nnOutputs = nnOutputs.numpy()\n        nnOutputs = nnOutputs[0]\n        nnOutputs = nnOutputs - np.max(nnOutputs)\n        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n        f1.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n        \n        # Using temperature scaling\n        outputs = outputs / temper\n        \n        # Calculating the perturbation we need to add, that is,\n        # the sign of gradient of cross entropy loss w.r.t. input\n        maxIndexTemp = np.argmax(nnOutputs)\n        labels = Variable(torch.LongTensor([maxIndexTemp]).cuda(CUDA_DEVICE))\n        loss = criterion(outputs, labels)\n        loss.backward()\n        \n        \n        # Normalizing the gradient to binary in {0, 1}\n        gradient =  (torch.ge(inputs.grad.data, 0))\n        gradient = (gradient.float() - 0.5) * 2\n        # Normalizing the gradient to the same space of image\n        gradient[0][0] = (gradient[0][0] )/(63.0/255.0)\n        gradient[0][1] = (gradient[0][1] )/(62.1/255.0)\n        gradient[0][2] = (gradient[0][2])/(66.7/255.0)\n        # Adding small perturbations to images\n        tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\n        outputs = net1(Variable(tempInputs))\n        outputs = outputs / temper\n        # Calculating the confidence after adding perturbations\n        nnOutputs = outputs.data.cpu()\n        nnOutputs = nnOutputs.numpy()\n        nnOutputs = nnOutputs[0]\n        nnOutputs = nnOutputs - np.max(nnOutputs)\n        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n\n        g1.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n        if j % 100 == 99:\n            print(\"{:4}/{:4}  images processed, {:.1f} seconds used.\".format(j+1-1000, N-1000, time.time()-t0))\n            t0 = time.time()\n\n\n\n########################################Out-of-Distribution######################################\n    print(\"Processing out-of-distribution images\")\n    for j, data in enumerate(testloader_out):\n        if j<1000: continue\n        \n        images = torch.rand(1,3,32,32)\n        images[0][0] = (images[0][0] - 125.3/255) / (63.0/255)\n        images[0][1] = (images[0][1] - 123.0/255) / (62.1/255)\n        images[0][2] = (images[0][2] - 113.9/255) / (66.7/255)\n        \n        \n        inputs = Variable(images.cuda(CUDA_DEVICE), requires_grad = True)\n        outputs = net1(inputs)\n        \n        \n        \n        # Calculating the confidence of the output, no perturbation added here\n        nnOutputs = outputs.data.cpu()\n        nnOutputs = nnOutputs.numpy()\n        nnOutputs = nnOutputs[0]\n        nnOutputs = nnOutputs - np.max(nnOutputs)\n        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n        f2.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n        \n        # Using temperature scaling\n        outputs = outputs / temper\n        \n        # Calculating the perturbation we need to add, that is,\n        # the sign of gradient of cross entropy loss w.r.t. input\n        maxIndexTemp = np.argmax(nnOutputs)\n        labels = Variable(torch.LongTensor([maxIndexTemp]).cuda(CUDA_DEVICE))\n        loss = criterion(outputs, labels)\n        loss.backward()\n        \n        # Normalizing the gradient to binary in {0, 1}\n        gradient =  (torch.ge(inputs.grad.data, 0))\n        gradient = (gradient.float() - 0.5) * 2\n        # Normalizing the gradient to the same space of image\n        gradient[0][0] = (gradient[0][0] )/(63.0/255.0)\n        gradient[0][1] = (gradient[0][1] )/(62.1/255.0)\n        gradient[0][2] = (gradient[0][2])/(66.7/255.0)\n        # Adding small perturbations to images\n        tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\n        outputs = net1(Variable(tempInputs))\n        outputs = outputs / temper\n        # Calculating the confidence after adding perturbations\n        nnOutputs = outputs.data.cpu()\n        nnOutputs = nnOutputs.numpy()\n        nnOutputs = nnOutputs[0]\n        nnOutputs = nnOutputs - np.max(nnOutputs)\n        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n        g2.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n        if j % 100 == 99:\n            print(\"{:4}/{:4} images processed, {:.1f} seconds used.\".format(j+1-1000, N-1000, time.time()-t0))\n            t0 = time.time()\n\n        if j== N-1: break\n","metadata":{"execution":{"iopub.status.busy":"2022-11-17T19:27:07.879711Z","iopub.execute_input":"2022-11-17T19:27:07.880061Z","iopub.status.idle":"2022-11-17T19:27:07.898485Z","shell.execute_reply.started":"2022-11-17T19:27:07.880032Z","shell.execute_reply":"2022-11-17T19:27:07.897348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python main.py --nn DenseNet_Train_MNIST_99.65_accuracy --in_dataset MNIST --out_dataset FashionMNIST --magnitude 0.0014 --temperature 1000 --gpu 0","metadata":{"papermill":{"duration":989.596984,"end_time":"2022-11-17T17:54:07.515866","exception":false,"start_time":"2022-11-17T17:37:37.918882","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-11-17T19:27:12.067701Z","iopub.execute_input":"2022-11-17T19:27:12.068312Z","iopub.status.idle":"2022-11-17T19:27:23.800618Z","shell.execute_reply.started":"2022-11-17T19:27:12.068247Z","shell.execute_reply":"2022-11-17T19:27:23.799440Z"},"trusted":true},"execution_count":null,"outputs":[]}]}